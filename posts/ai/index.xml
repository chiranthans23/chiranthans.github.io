<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Artificial Intelligence on Chiranthan's Blog</title><link>https://blog.chiranthans23.com/posts/ai/</link><description>Recent content in Artificial Intelligence on Chiranthan's Blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 04 Sep 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://blog.chiranthans23.com/posts/ai/index.xml" rel="self" type="application/rss+xml"/><item><title>Stress analysis on Social media</title><link>https://blog.chiranthans23.com/posts/ai/stress-analysis-social-media/</link><pubDate>Sat, 04 Sep 2021 00:00:00 +0000</pubDate><guid>https://blog.chiranthans23.com/posts/ai/stress-analysis-social-media/</guid><description>Resources Code: https://github.com/chiranthans23/stress-analysis-social-media Train and test data: dreaddit-train.csv, dreaddit-test.csv EDA report: report.html Introduction This is a work on Dreaddit dataset taken from one of latest datasets on Kaggle bwhich is created using data from 5 different Reddit communities. This is a slightly different problem as the length of the posts are larger than the usual tweets. Additionally, the data available for training is also not quite huge.
First look This work doesn&amp;rsquo;t really have a extensive EDA, but just looks at the dataset - columns, amount of data available, missing values.</description></item></channel></rss>